{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Netork in 5 easy steps\n",
    "1. Import Keras modules\n",
    "2. Load dataset and preprocess\n",
    "3. Build Keras model\n",
    "4. Train the model using the training set\n",
    "5. Test the best trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Import Keras modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Load dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 training samples\n",
      "10000 testing samples\n"
     ]
    }
   ],
   "source": [
    "## Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0],1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0],1, 28, 28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "## Preprocess image pixels elem-wise.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'training samples')\n",
    "print(X_test.shape[0], 'testing samples')\n",
    "\n",
    "## Convert class into 1-hot encoding.\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "## Global vars to build the model\n",
    "input_dim = X_train.shape[2]\n",
    "hidden_units = int(input_dim/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Build keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "## input: 28x28 images with 1 channels -> (1, 28, 28) tensors.\n",
    "## this applies 20 convolution filters of size 2x2 each.\n",
    "model.add(Convolution2D(20, 2, 2, border_mode='valid', input_shape=(1, input_dim, input_dim)), )\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(20, 2, 2))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "## 40 convolutional filters of size 2x2 each\n",
    "model.add(Convolution2D(40, 2, 2, border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(40, 2, 2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "# Note: Keras does automatic shape inference.\n",
    "model.add(Dense(784))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Train the model using the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 183s - loss: 0.1977 - acc: 0.6615 - val_loss: 0.0501 - val_acc: 0.9125\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 174s - loss: 0.0453 - acc: 0.9213 - val_loss: 0.0295 - val_acc: 0.9506\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 170s - loss: 0.0295 - acc: 0.9492 - val_loss: 0.0214 - val_acc: 0.9649\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 185s - loss: 0.0214 - acc: 0.9633 - val_loss: 0.0153 - val_acc: 0.9763\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 187s - loss: 0.0165 - acc: 0.9723 - val_loss: 0.0128 - val_acc: 0.9786\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 192s - loss: 0.0135 - acc: 0.9776 - val_loss: 0.0114 - val_acc: 0.9815\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 188s - loss: 0.0111 - acc: 0.9811 - val_loss: 0.0103 - val_acc: 0.9820\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 182s - loss: 0.0098 - acc: 0.9838 - val_loss: 0.0093 - val_acc: 0.9847\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 169s - loss: 0.0083 - acc: 0.9855 - val_loss: 0.0095 - val_acc: 0.9849\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 176s - loss: 0.0075 - acc: 0.9872 - val_loss: 0.0093 - val_acc: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11fe26550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the model using 1000 random samples\n",
    "per epoch and using 20% of the training set \n",
    "as validation set during training.\n",
    "\"\"\" \n",
    "model.fit(X_train, Y_train, verbose=1, batch_size=1000, validation_split=0.2, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5. Test the best trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 9s     \n",
      "Test error is 1.3600003719329834\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, batch_size=1000)\n",
    "test_error = 100 - (score[1] * 100)\n",
    "print('Test error is {}'.format(test_error))\n",
    "\n",
    "##### NOTE!!! #####\n",
    "## This is not the best MNIST CNN model in the world. \n",
    "## There are still other tricks which you can \n",
    "## do to improve this model... \n",
    "## But hey you, just managed to run CNN! Great Job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### You can check how good this model is by checking this link.\n",
    "\n",
    "(http://yann.lecun.com/exdb/mnist/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
